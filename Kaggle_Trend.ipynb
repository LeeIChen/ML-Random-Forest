{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 路徑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import threading # 因為等等要載入大量資料所以要multi thread\n",
    "from multiprocessing import Queue\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier, GradientBoostingClassifier,ExtraTreesClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import loadtxt\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.rcParams['font.family']='SimHei' #顯示中文\n",
    "\n",
    "#特殊符號: | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport gc\\ndel big_object\\ngc.collect()\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import gc\n",
    "del big_object\n",
    "gc.collect()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 載入資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [FileID, label]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [FileID, label]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# 載入資料 A\n",
    "\n",
    "train = pd.read_csv('training-set.csv', encoding = \"utf-8\", header=None)\n",
    "test  = pd.read_csv('testing-set.csv', encoding = \"utf-8\", header=None)\n",
    "\n",
    "train_exc = pd.read_csv('exception_train.txt', encoding = \"utf-8\", header=None)\n",
    "test_exc = pd.read_csv('exception_testing.txt', encoding = \"utf-8\", header=None)\n",
    "\n",
    "train.columns=['FileID','label']\n",
    "test.columns=['FileID','label']\n",
    "\n",
    "\n",
    "# 確認排除的FileID在training set裡面找不到\n",
    "\n",
    "for item in train_exc:\n",
    "    print(train[train['FileID']==item])\n",
    "    \n",
    "for item in test_exc:\n",
    "    print(test[test['FileID']==item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             FileID  label  batch\n",
      "0  0000e2398b12121a85166fed5fe2a3da      0      1\n",
      "1  0001fe8dce14ce099aa6ca8ea5026ea7      0      1\n",
      "2  00027f50019000accc492e5684efc818      0      1\n",
      "                                 FileID  label  batch\n",
      "52507  fff57a378fbc3bc458b7cc9171ce7c31      0     10\n",
      "52508  fff6851ddbf675b950e1184c988b984e      1     10\n",
      "52509  fff6e0c69b020e121bce461aa1b4e447      1     10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(52510, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 將 FileID 切割，免得等等跑不動\n",
    "\n",
    "train = train.sort_values('FileID')\n",
    "\n",
    "n = 10\n",
    "a = 0\n",
    "b = int(len(train)/n)\n",
    "\n",
    "train1 = pd.DataFrame()\n",
    "\n",
    "for i in range(1, n+1):\n",
    "    tr = train.iloc[a:b, :]\n",
    "    tr['batch'] = i\n",
    "    a = b\n",
    "    b = b + int(len(train)/n)\n",
    "    train1 = train1.append(tr)\n",
    "\n",
    "print(train1.head(3))\n",
    "print(train1.tail(3))\n",
    "train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入資料 B \n",
    "\n",
    "import glob\n",
    "path = '/data/examples/trend/data/query_log'\n",
    "\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\")) # advisable to use os.path.join as this makes concatenation OS independent\n",
    "file = [pd.read_csv(f, names=['FileID','CustomerID','QueryTs','ProductID']) for f in all_files]\n",
    "raw  = pd.concat(file, ignore_index=True)\n",
    "\n",
    "print(raw.shape)\n",
    "raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FileID</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>QueryTs</th>\n",
       "      <th>ProductID</th>\n",
       "      <th>cnt</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>hour</th>\n",
       "      <th>week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>83273105</th>\n",
       "      <td>ad141ec00374be238c2476b2f2c499e4</td>\n",
       "      <td>2956a9498da8ad0ce55015ce67ef4693</td>\n",
       "      <td>1493251196</td>\n",
       "      <td>55649</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-04-26</td>\n",
       "      <td>23:59:56</td>\n",
       "      <td>23</td>\n",
       "      <td>Wednesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83273106</th>\n",
       "      <td>f62bfd843961a7c4a69f403963867966</td>\n",
       "      <td>5065b2835b355439c2b765f22e2702f4</td>\n",
       "      <td>1493251196</td>\n",
       "      <td>55649</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-04-26</td>\n",
       "      <td>23:59:56</td>\n",
       "      <td>23</td>\n",
       "      <td>Wednesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83273107</th>\n",
       "      <td>e612c3a78dc931c7693327b93c22b5de</td>\n",
       "      <td>f09c46986311323fc3b75e2e6bf15688</td>\n",
       "      <td>1493251198</td>\n",
       "      <td>55649</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-04-26</td>\n",
       "      <td>23:59:58</td>\n",
       "      <td>23</td>\n",
       "      <td>Wednesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83273108</th>\n",
       "      <td>1a64991c9ef66ec6332262c26b5303b5</td>\n",
       "      <td>d6bfafcb8123cc30c5a9c1a9b3a43a06</td>\n",
       "      <td>1493251199</td>\n",
       "      <td>55649</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-04-26</td>\n",
       "      <td>23:59:59</td>\n",
       "      <td>23</td>\n",
       "      <td>Wednesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83273109</th>\n",
       "      <td>f62bfd843961a7c4a69f403963867966</td>\n",
       "      <td>8a3c4f62c058fd5d1ae4e174e7d4bf35</td>\n",
       "      <td>1493251199</td>\n",
       "      <td>55649</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-04-26</td>\n",
       "      <td>23:59:59</td>\n",
       "      <td>23</td>\n",
       "      <td>Wednesday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    FileID                        CustomerID  \\\n",
       "83273105  ad141ec00374be238c2476b2f2c499e4  2956a9498da8ad0ce55015ce67ef4693   \n",
       "83273106  f62bfd843961a7c4a69f403963867966  5065b2835b355439c2b765f22e2702f4   \n",
       "83273107  e612c3a78dc931c7693327b93c22b5de  f09c46986311323fc3b75e2e6bf15688   \n",
       "83273108  1a64991c9ef66ec6332262c26b5303b5  d6bfafcb8123cc30c5a9c1a9b3a43a06   \n",
       "83273109  f62bfd843961a7c4a69f403963867966  8a3c4f62c058fd5d1ae4e174e7d4bf35   \n",
       "\n",
       "             QueryTs ProductID  cnt        date      time  hour       week  \n",
       "83273105  1493251196     55649    1  2017-04-26  23:59:56    23  Wednesday  \n",
       "83273106  1493251196     55649    1  2017-04-26  23:59:56    23  Wednesday  \n",
       "83273107  1493251198     55649    1  2017-04-26  23:59:58    23  Wednesday  \n",
       "83273108  1493251199     55649    1  2017-04-26  23:59:59    23  Wednesday  \n",
       "83273109  1493251199     55649    1  2017-04-26  23:59:59    23  Wednesday  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw['cnt'] = 1\n",
    "raw['date'] = pd.to_datetime(raw['QueryTs'], unit='s').dt.date\n",
    "raw['time'] = pd.to_datetime(raw['QueryTs'], unit='s').dt.time\n",
    "raw['hour'] = pd.to_datetime(raw['QueryTs'], unit='s').dt.hour\n",
    "raw['week'] = pd.to_datetime(raw['QueryTs'], unit='s').dt.weekday_name\n",
    "\n",
    "raw.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83273110, 9) (52518, 2) (54242074, 11)\n",
      "['7acab3' '55649' '055649' '634e6b' 'c76d58' 'c105a0' 'e47f04' '885fab'\n",
      " '26a5d0' 'a310bb' 'dd8d4a' 'd465fc' '533133' '262880' 'b93794' '8541a0'\n",
      " '218578' '3ea8c3' '05b409' '20f8a5' '0374c4' 'cc3a6a' '8452da' 'aaa9c8'\n",
      " '0cdb7a' '3c2be6' '75f310' 'fec24f']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FileID</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>QueryTs</th>\n",
       "      <th>ProductID</th>\n",
       "      <th>cnt</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>hour</th>\n",
       "      <th>week</th>\n",
       "      <th>label</th>\n",
       "      <th>batch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6872637</th>\n",
       "      <td>f096e1c6e0cbaf10389fbf427b4d341f</td>\n",
       "      <td>0000006fa286976bf35ea17f1f19bc7a</td>\n",
       "      <td>1493364274</td>\n",
       "      <td>7acab3</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-04-28</td>\n",
       "      <td>07:24:34</td>\n",
       "      <td>7</td>\n",
       "      <td>Friday</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6872918</th>\n",
       "      <td>f096e1c6e0cbaf10389fbf427b4d341f</td>\n",
       "      <td>0000006fa286976bf35ea17f1f19bc7a</td>\n",
       "      <td>1493531993</td>\n",
       "      <td>7acab3</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-04-30</td>\n",
       "      <td>05:59:53</td>\n",
       "      <td>5</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119508</th>\n",
       "      <td>19308434813502167aaef38f578981a4</td>\n",
       "      <td>00000145d9062eada528bace5fb4864e</td>\n",
       "      <td>1490544224</td>\n",
       "      <td>7acab3</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-03-26</td>\n",
       "      <td>16:03:44</td>\n",
       "      <td>16</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32148560</th>\n",
       "      <td>ee6a1280be5c96d7b2461de6b7578180</td>\n",
       "      <td>00000145d9062eada528bace5fb4864e</td>\n",
       "      <td>1492708112</td>\n",
       "      <td>7acab3</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-04-20</td>\n",
       "      <td>17:08:32</td>\n",
       "      <td>17</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32151096</th>\n",
       "      <td>ee6a1280be5c96d7b2461de6b7578180</td>\n",
       "      <td>00000145d9062eada528bace5fb4864e</td>\n",
       "      <td>1492962863</td>\n",
       "      <td>7acab3</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-04-23</td>\n",
       "      <td>15:54:23</td>\n",
       "      <td>15</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    FileID                        CustomerID  \\\n",
       "6872637   f096e1c6e0cbaf10389fbf427b4d341f  0000006fa286976bf35ea17f1f19bc7a   \n",
       "6872918   f096e1c6e0cbaf10389fbf427b4d341f  0000006fa286976bf35ea17f1f19bc7a   \n",
       "119508    19308434813502167aaef38f578981a4  00000145d9062eada528bace5fb4864e   \n",
       "32148560  ee6a1280be5c96d7b2461de6b7578180  00000145d9062eada528bace5fb4864e   \n",
       "32151096  ee6a1280be5c96d7b2461de6b7578180  00000145d9062eada528bace5fb4864e   \n",
       "\n",
       "             QueryTs ProductID  cnt        date      time  hour      week  \\\n",
       "6872637   1493364274    7acab3    1  2017-04-28  07:24:34     7    Friday   \n",
       "6872918   1493531993    7acab3    1  2017-04-30  05:59:53     5    Sunday   \n",
       "119508    1490544224    7acab3    1  2017-03-26  16:03:44    16    Sunday   \n",
       "32148560  1492708112    7acab3    1  2017-04-20  17:08:32    17  Thursday   \n",
       "32151096  1492962863    7acab3    1  2017-04-23  15:54:23    15    Sunday   \n",
       "\n",
       "          label  batch  \n",
       "6872637       0     10  \n",
       "6872918       0     10  \n",
       "119508        0      1  \n",
       "32148560      0     10  \n",
       "32151096      0     10  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw = pd.merge(raw, train1, on=['FileID'])\n",
    "train_raw['ProductID'] = train_raw['ProductID'].astype(str)   # 因為值中有數字文字\n",
    "train_raw.replace({'ProductID':{'55649': '055649'}})\n",
    "train_raw = train_raw.sort_values(['CustomerID','FileID','QueryTs'])\n",
    "\n",
    "print(raw.shape, train.shape, train_raw.shape)\n",
    "print(train_raw.ProductID.unique())\n",
    "train_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "observe1 = train_raw.groupby(['FileID','CustomerID','ProductID'])[['cnt','label']].sum()\n",
    "observe1 = observe1.sort_values('label', ascending=0)\n",
    "aa = observe1.loc[observe1['label']==1]\n",
    "aa.head()\n",
    "\n",
    "observe2 = train_raw.groupby(['week'])[['cnt','label']].sum() \n",
    "observe2['fr_rate'] = observe2['label'] / observe2['cnt']\n",
    "observe2 = observe2.sort_values('fr_rate', ascending=0)\n",
    "print(observe2)\n",
    "\n",
    "observe3 = train_raw.groupby(['ProductID'])[['cnt','label']].sum() \n",
    "observe3['fr_rate'] = observe3['label'] / observe3['cnt']\n",
    "observe3 = observe3.sort_values('fr_rate', ascending=0)\n",
    "print(observe3)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 其他常用資料整理\n",
    "'''\n",
    "for dfB in full_data:  \n",
    "    dfB['ColumnA'] = dfB['ColumnA'].fillna(dfA['ColumnA'].median())#刪除空值，用中位數代替  \n",
    "\n",
    "df['columnB'] = pd.qcut(df['columnA'], 4)  #分為四等份\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2A. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FileID</th>\n",
       "      <th>DayFilMax</th>\n",
       "      <th>DayFilMin</th>\n",
       "      <th>DayFilMea</th>\n",
       "      <th>DayTCsMax</th>\n",
       "      <th>DayTCsMin</th>\n",
       "      <th>DayTCsMea</th>\n",
       "      <th>DaySCsMax</th>\n",
       "      <th>DaySCsMin</th>\n",
       "      <th>DaySCsMea</th>\n",
       "      <th>DayTPrMax</th>\n",
       "      <th>DayTPrMin</th>\n",
       "      <th>DayTPrMea</th>\n",
       "      <th>DaySPrMax</th>\n",
       "      <th>DaySPrMin</th>\n",
       "      <th>DaySPrMea</th>\n",
       "      <th>TCs</th>\n",
       "      <th>TPr</th>\n",
       "      <th>Day</th>\n",
       "      <th>Fil</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000e2398b12121a85166fed5fe2a3da</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>15.666667</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>11.750000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>11.750000</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001fe8dce14ce099aa6ca8ea5026ea7</td>\n",
       "      <td>66</td>\n",
       "      <td>3</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>66</td>\n",
       "      <td>3</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>234</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00027f50019000accc492e5684efc818</td>\n",
       "      <td>253</td>\n",
       "      <td>8</td>\n",
       "      <td>75.600000</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>117</td>\n",
       "      <td>1</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>137</td>\n",
       "      <td>1</td>\n",
       "      <td>23.625000</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00028c9da3573ec50db74b44310ae507</td>\n",
       "      <td>339</td>\n",
       "      <td>4</td>\n",
       "      <td>61.333333</td>\n",
       "      <td>107</td>\n",
       "      <td>3</td>\n",
       "      <td>20.833333</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>2.944000</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>271</td>\n",
       "      <td>1</td>\n",
       "      <td>24.533333</td>\n",
       "      <td>113</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0003dc8130969abe688cadf5f14ea19f</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>39.166667</td>\n",
       "      <td>75</td>\n",
       "      <td>1</td>\n",
       "      <td>27.500000</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1.424242</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>14.687500</td>\n",
       "      <td>130</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             FileID  DayFilMax  DayFilMin  DayFilMea  \\\n",
       "0  0000e2398b12121a85166fed5fe2a3da         43          1  15.666667   \n",
       "1  0001fe8dce14ce099aa6ca8ea5026ea7         66          3  39.000000   \n",
       "2  00027f50019000accc492e5684efc818        253          8  75.600000   \n",
       "3  00028c9da3573ec50db74b44310ae507        339          4  61.333333   \n",
       "4  0003dc8130969abe688cadf5f14ea19f        115          1  39.166667   \n",
       "\n",
       "   DayTCsMax  DayTCsMin  DayTCsMea  DaySCsMax  DaySCsMin  DaySCsMea  \\\n",
       "0          2          1   1.333333         38          1  11.750000   \n",
       "1         66          3  39.000000          1          1   1.000000   \n",
       "2         11          1   6.000000        117          1  12.600000   \n",
       "3        107          3  20.833333         55          1   2.944000   \n",
       "4         75          1  27.500000         18          1   1.424242   \n",
       "\n",
       "   DayTPrMax  DayTPrMin  DayTPrMea  DaySPrMax  DaySPrMin  DaySPrMea  TCs  TPr  \\\n",
       "0          2          1   1.333333         38          1  11.750000    3    2   \n",
       "1          4          1   3.000000         45          1  13.000000  234    4   \n",
       "2          5          1   3.200000        137          1  23.625000   22    6   \n",
       "3          5          1   2.500000        271          1  24.533333  113    5   \n",
       "4          5          1   2.666667         87          1  14.687500  130    5   \n",
       "\n",
       "   Day  Fil  \n",
       "0    3   47  \n",
       "1    6  234  \n",
       "2    5  378  \n",
       "3    6  368  \n",
       "4    6  235  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aggregate\n",
    "\n",
    "train_agg = pd.DataFrame()\n",
    "for i in range(10):\n",
    "    \n",
    "    tr = train_raw.loc[train_raw['batch'] == i+1]\n",
    "\n",
    "    # 單日File被使用幾次\n",
    "    DayFil    = tr.groupby(['FileID', 'date']).size()\n",
    "    DayFilMax = DayFil.groupby(level=0).max()\n",
    "    DayFilMin = DayFil.groupby(level=0).min()\n",
    "    DayFilMea = DayFil.groupby(level=0).mean()\n",
    "\n",
    "\n",
    "    DayCs  = tr.groupby(['FileID', 'date', 'CustomerID']).size() # 可以先拿掉\n",
    "    DayPr  = tr.groupby(['FileID', 'date', 'ProductID']).size()\n",
    "    Pr     = tr.groupby(['FileID', 'ProductID']).size()\n",
    "    Cs     = tr.groupby(['FileID', 'CustomerID']).size()\n",
    "    Day    = tr.groupby(['FileID', 'date']).size()\n",
    "\n",
    "    # 單日File被多少客人使用\n",
    "    DayTCs    = DayCs.groupby(level=(0,1)).size()\n",
    "    DayTCsMax = DayTCs.groupby(level=0).max()\n",
    "    DayTCsMin = DayTCs.groupby(level=0).min()\n",
    "    DayTCsMea = DayTCs.groupby(level=0).mean()\n",
    "\n",
    "    # 單日File被同一客人使用幾次\n",
    "    DaySCsMax = DayCs.groupby(level=0).max()\n",
    "    DaySCsMin = DayCs.groupby(level=0).min()\n",
    "    DaySCsMea = DayCs.groupby(level=0).mean()\n",
    "\n",
    "    # 單日File被多少產品使用\n",
    "    DayTPr    = DayPr.groupby(level=(0,1)).size()\n",
    "    DayTPrMax = DayTPr.groupby(level=0).max()\n",
    "    DayTPrMin = DayTPr.groupby(level=0).min()\n",
    "    DayTPrMea = DayTPr.groupby(level=0).mean()\n",
    "\n",
    "    # 單日File被同一產品使用幾次\n",
    "    DaySPrMax = DayPr.groupby(level=0).max()\n",
    "    DaySPrMin = DayPr.groupby(level=0).min()\n",
    "    DaySPrMea = DayPr.groupby(level=0).mean()\n",
    "\n",
    "\n",
    "    TCs    = Cs.groupby(level=(0)).size() # 總共File被多少客人使用\n",
    "    TPr    = Pr.groupby(level=(0)).size() # 總共File被多少產品使用\n",
    "    Day    = Day.groupby(level=(0)).size() # 總共File被使用幾天\n",
    "    Fil    = tr.groupby(['FileID']).size() # 總共File被使用幾次\n",
    "\n",
    "    train_a = pd.concat([DayFilMax, DayFilMin, DayFilMea,\n",
    "                           DayTCsMax, DayTCsMin, DayTCsMea,\n",
    "                           DaySCsMax, DaySCsMin, DaySCsMea,\n",
    "                           DayTPrMax, DayTPrMin, DayTPrMea,\n",
    "                           DaySPrMax, DaySPrMin, DaySPrMea,\n",
    "                           TCs,    TPr,   Day,    Fil,     ], axis=1)\n",
    "\n",
    "    train_agg = train_agg.append(train_a)\n",
    "    print(i+1)\n",
    "    \n",
    "train_agg.columns = ['DayFilMax', 'DayFilMin', 'DayFilMea',\n",
    "                     'DayTCsMax', 'DayTCsMin', 'DayTCsMea',\n",
    "                     'DaySCsMax', 'DaySCsMin', 'DaySCsMea',\n",
    "                     'DayTPrMax', 'DayTPrMin', 'DayTPrMea',\n",
    "                     'DaySPrMax', 'DaySPrMin', 'DaySPrMea',\n",
    "                     'TCs',   'TPr',  'Day',  'Fil'      ]\n",
    "\n",
    "train_agg = pd.DataFrame(train_agg).reset_index()\n",
    "train_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52510, 20)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_agg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0374c4</th>\n",
       "      <th>055649</th>\n",
       "      <th>05b409</th>\n",
       "      <th>0cdb7a</th>\n",
       "      <th>20f8a5</th>\n",
       "      <th>218578</th>\n",
       "      <th>262880</th>\n",
       "      <th>26a5d0</th>\n",
       "      <th>3c2be6</th>\n",
       "      <th>3ea8c3</th>\n",
       "      <th>...</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>Friday</th>\n",
       "      <th>Monday</th>\n",
       "      <th>Saturday</th>\n",
       "      <th>Sunday</th>\n",
       "      <th>Thursday</th>\n",
       "      <th>Tuesday</th>\n",
       "      <th>Wednesday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>253.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>339.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>115.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0374c4  055649  05b409  0cdb7a  20f8a5  218578  262880  26a5d0  3c2be6  \\\n",
       "0     0.0    38.0     NaN     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1     0.0    44.0     NaN     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2     0.0    11.0     NaN     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3     0.0     3.0     NaN     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4     0.0    20.0     NaN     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   3ea8c3    ...       21    22    23  Friday  Monday  Saturday  Sunday  \\\n",
       "0     0.0    ...      0.0   0.0   0.0     0.0     0.0      43.0     3.0   \n",
       "1     0.0    ...      6.0  12.0  21.0    66.0    48.0      33.0    45.0   \n",
       "2   117.0    ...      7.0  17.0   6.0    80.0     0.0       8.0     0.0   \n",
       "3    64.0    ...      2.0   1.0   4.0     0.0     4.0     339.0     6.0   \n",
       "4    38.0    ...      0.0   7.0   8.0     7.0     0.0       2.0     1.0   \n",
       "\n",
       "   Thursday Tuesday  Wednesday  \n",
       "0       0.0     0.0        1.0  \n",
       "1      39.0     0.0        3.0  \n",
       "2      27.0    10.0      253.0  \n",
       "3       9.0     4.0        6.0  \n",
       "4      50.0    60.0      115.0  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot encoding\n",
    "\n",
    "train1 = pd.DataFrame()\n",
    "train2 = pd.DataFrame()\n",
    "train3 = pd.DataFrame()\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    tr = train_raw.loc[train_raw['batch'] == i+1]\n",
    "\n",
    "    Pr = tr[['FileID', 'ProductID']]\n",
    "    Hr = tr[['FileID', 'hour']]\n",
    "    Wk = tr[['FileID', 'week']]\n",
    "\n",
    "    train_prd = pd.concat([Pr, pd.get_dummies(Pr.ProductID)], 1).groupby(['FileID']).sum().reset_index()\n",
    "    train_hur = pd.concat([Hr, pd.get_dummies(Hr.hour)], 1).groupby(['FileID']).sum().reset_index()\n",
    "    train_wek = pd.concat([Wk, pd.get_dummies(Wk.week)], 1).groupby(['FileID']).sum().reset_index()\n",
    "\n",
    "    '''\n",
    "    train_dum = pd.concat([all_v2[['FileID','CustomerID']], train_prd, train_hur, train_wek], axis=1) \n",
    "    train_dum2= train_dum.groupby(['FileID']).sum()\n",
    "    train_dum3= train_dum.groupby(['FileID']).std()\n",
    "    '''\n",
    "    \n",
    "    train1 = train1.append(train_prd)\n",
    "    train2 = train2.append(train_hur)\n",
    "    train3 = train3.append(train_wek)\n",
    "    \n",
    "    print(i+1)\n",
    "    \n",
    "train_dum = pd.merge(train1, train2, on='FileID')\n",
    "train_dum = pd.merge(train_dum, train3, on='FileID')\n",
    "\n",
    "train_dum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找回regular_X\n",
    "# 新增300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:49: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# 使用間隔\n",
    "\n",
    "def regular(x):\n",
    "    return ( x%3600 <= 10 ) | ( x%3600 >= 3590 )\n",
    "\n",
    "#print(regular(np.array([1,3599,3601,450,720,4150])))\n",
    "\n",
    "def outlier(x):\n",
    "    Q1 = np.percentile(x, 25)\n",
    "    Q3 = np.percentile(x, 75)\n",
    "    h = Q3 - Q1\n",
    "    return ((Q3 + 0.5*h) >= x) & (x >= (Q1 - 0.5*h))\n",
    "\n",
    "#print(outlier(np.array([1,100,100,100,100,100,100])))\n",
    "\n",
    "train1 = pd.DataFrame()\n",
    "train2 = pd.DataFrame()\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    tr = train_raw.loc[train_raw['batch'] == i+1]\n",
    "\n",
    "    tr = tr[['CustomerID','FileID','QueryTs','label']]\n",
    "\n",
    "    tr['Delta1']    = tr.groupby(['FileID'])['QueryTs'].diff(1) # diff 1 or 2 or n : 差幾個interval\n",
    "    tr['Delta1_d']  = tr.groupby(['FileID'])['Delta1'].diff(1)\n",
    "    tr['Delta1_d']  = abs(tr.groupby(['FileID'])['Delta1'].diff(1)) # 絕對值\n",
    "    \n",
    "    tr['Rank']      = tr.groupby(['FileID'])['QueryTs'].rank() # 順序排名\n",
    "\n",
    "    #crosstab 也可以 (樞紐)\n",
    "\n",
    "    #train_freq['Delta1']    = train_freq.groupby(['FileID'])['QueryTs'].transform(lambda x: x-x.shift(1))\n",
    "    #train_freq['Delta1_d']  = train_freq.groupby(['FileID'])['Delta1'].transform(lambda x: abs(x-x.shift(1)))\n",
    "    #train_freq['Delta2']    = train_freq.groupby(['CustomerID','FileID'])['QueryTs'].transform(lambda x: x-x.shift(1))\n",
    "    #train_freq['Delta2_d']  = train_freq.groupby(['CustomerID','FileID'])['Delta2'].transform(lambda x: abs(x-x.shift(1)))\n",
    "\n",
    "    # 測: 是否為3600秒一單位)\n",
    "\n",
    "    train_freq1 = tr.drop(['Delta1_d'], axis=1).dropna() #.loc[train_freq['label']==0]\n",
    "    train_freq1['Regular'] = train_freq1['Delta1'].apply(regular)\n",
    "    train_freq1mean = train_freq1.groupby(['FileID'])['Regular'].mean().reset_index()\n",
    "    train_freq1max  = train_freq1.groupby(['FileID'])['Regular'].max().reset_index()\n",
    "    train_freq1 = pd.merge(train_freq1mean, train_freq1max, on=['FileID'], how='outer')\n",
    "\n",
    "    # 測: 規律性 (標準差)\n",
    "\n",
    "    train_freq2 = tr.dropna() # .loc[train_freq['label']==0]\n",
    "    train_freq2['Delta_OL'] = train_freq2.groupby(['CustomerID','FileID'])['Delta1_d'].transform(outlier)\n",
    "    train_freq2 = train_freq2[train_freq2['Delta_OL'] != 0]\n",
    "    train_freq2['Delta_sd'] = train_freq2.groupby(['CustomerID','FileID'])['Delta1_d'].transform(lambda x: np.std(x)/np.mean(x))\n",
    "    train_freq2mean = train_freq2.groupby(['FileID'])['Delta_sd'].mean().fillna(0).reset_index()\n",
    "    train_freq2max  = train_freq2.groupby(['FileID'])['Delta_sd'].max().fillna(0).reset_index()\n",
    "    train_freq2 = pd.merge(train_freq2mean, train_freq2max, on=['FileID'], how='outer')\n",
    "\n",
    "    train1 = train1.append(train_freq1)\n",
    "    train2 = train2.append(train_freq2)\n",
    "    print(i+1)\n",
    "\n",
    "train_freq = pd.merge(train1, train2, on='FileID')\n",
    "train_freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_freq, train_dum.shape, train_agg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2B. 資料清整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全部合併\n",
    "\n",
    "train_all = pd.merge(train_freq, train_dum, on=['FileID'], how='outer')\n",
    "train_all = pd.merge(train_all, train_agg, on=['FileID'], how='outer')\n",
    "\n",
    "train_all.head()\n",
    "\n",
    "# index & column 相merge時: train_all = pd.merge(train_all, train_agg, how='left', left_on=['FileID'], right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_all.columns)\n",
    "train_all.to_csv(\"train_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(52510, 85)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read\n",
    "train_all = pd.read_csv(\"train_all_3m.csv\")\n",
    "train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                            FileID  Regular_x Regular_y  \\\n",
      "0           0  0000e2398b12121a85166fed5fe2a3da   0.847826      True   \n",
      "1           1  0001fe8dce14ce099aa6ca8ea5026ea7   0.004292      True   \n",
      "2           2  00027f50019000accc492e5684efc818   0.551724      True   \n",
      "3           3  00028c9da3573ec50db74b44310ae507   0.220708      True   \n",
      "4           4  0003dc8130969abe688cadf5f14ea19f   0.047009      True   \n",
      "\n",
      "   Delta_sd_x  Delta_sd_y  0374c4  055649  05b409  0cdb7a ...   DayTPrMax  \\\n",
      "0    0.720793    0.808290     0.0    38.0     NaN     0.0 ...           2   \n",
      "1    0.000000    0.000000     0.0    44.0     NaN     0.0 ...           4   \n",
      "2    1.462955    2.027292     0.0    11.0     NaN     0.0 ...           5   \n",
      "3    0.471363    1.294782     0.0     3.0     NaN     0.0 ...           5   \n",
      "4    0.298189    0.964996     0.0    20.0     NaN     0.0 ...           5   \n",
      "\n",
      "   DayTPrMin  DayTPrMea  DaySPrMax  DaySPrMin  DaySPrMea  TCs  TPr  Day  Fil  \n",
      "0          1   1.333333         38          1  11.750000    3    2    3   47  \n",
      "1          1   3.000000         45          1  13.000000  234    4    6  234  \n",
      "2          1   3.200000        137          1  23.625000   22    6    5  378  \n",
      "3          1   2.500000        271          1  24.533333  113    5    6  368  \n",
      "4          1   2.666667         87          1  14.687500  130    5    6  235  \n",
      "\n",
      "[5 rows x 85 columns]\n",
      "Unnamed: 0        0\n",
      "FileID            0\n",
      "Regular_x        62\n",
      "Regular_y        62\n",
      "Delta_sd_x       62\n",
      "Delta_sd_y       62\n",
      "0374c4        36757\n",
      "055649            0\n",
      "05b409        42008\n",
      "0cdb7a        15753\n",
      "20f8a5            0\n",
      "218578            0\n",
      "262880            0\n",
      "26a5d0            0\n",
      "3c2be6        42008\n",
      "3ea8c3            0\n",
      "533133            0\n",
      "55649             0\n",
      "634e6b            0\n",
      "75f310            0\n",
      "7acab3            0\n",
      "8452da         5251\n",
      "8541a0            0\n",
      "885fab            0\n",
      "a310bb            0\n",
      "aaa9c8        36757\n",
      "b93794            0\n",
      "c105a0            0\n",
      "c76d58            0\n",
      "cc3a6a         5251\n",
      "              ...  \n",
      "20                0\n",
      "21                0\n",
      "22                0\n",
      "23                0\n",
      "Friday            0\n",
      "Monday            0\n",
      "Saturday          0\n",
      "Sunday            0\n",
      "Thursday          0\n",
      "Tuesday           0\n",
      "Wednesday         0\n",
      "DayFilMax         0\n",
      "DayFilMin         0\n",
      "DayFilMea         0\n",
      "DayTCsMax         0\n",
      "DayTCsMin         0\n",
      "DayTCsMea         0\n",
      "DaySCsMax         0\n",
      "DaySCsMin         0\n",
      "DaySCsMea         0\n",
      "DayTPrMax         0\n",
      "DayTPrMin         0\n",
      "DayTPrMea         0\n",
      "DaySPrMax         0\n",
      "DaySPrMin         0\n",
      "DaySPrMea         0\n",
      "TCs               0\n",
      "TPr               0\n",
      "Day               0\n",
      "Fil               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 去除 outlier\n",
    "\n",
    "train_all.columns\n",
    "print(train_all.head())\n",
    "print(train_all.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# 補值 (interval: 同一file同一customer之間的時間差)\n",
    "\n",
    "train_all[['Regular_x']]  = train_all[['Regular_x']].fillna(value=0)    # 沒有interval\n",
    "train_all[['Regular_y']]  = train_all[['Regular_y']].fillna(value=0)    # 沒有interval\n",
    "train_all[['Delta_sd_x']] = train_all[['Delta_sd_x']].fillna(value=100) # 沒有兩個以上interval\n",
    "train_all[['Delta_sd_y']] = train_all[['Delta_sd_y']].fillna(value=100) # 沒有兩個以上interval\n",
    "train_all                 = train_all.fillna(value=0)                   # 將所有產品補上0\n",
    "\n",
    "check1 = len(train_all[train_all['Regular_x'].isnull()])\n",
    "check2 = len(train_all[train_all['Regular_y'].isnull()])\n",
    "check3 = len(train_all[train_all['Delta_sd_x'].isnull()])\n",
    "check4 = len(train_all[train_all['Delta_sd_y'].isnull()])\n",
    "check5 = len(train_all[train_all['20f8a5'].isnull()])\n",
    "check6 = len(train_all[train_all['DayTCsMea'].isnull()])\n",
    "\n",
    "print(check1)\n",
    "print(check2)\n",
    "print(check3)\n",
    "print(check4)\n",
    "print(check5)\n",
    "print(check6)\n",
    "\n",
    "train_all[['Regular_y']] = train_all[['Regular_y']].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalization\n",
    "'''\n",
    "def normalize(x, axis, method, minmax_range =(0,1)):\n",
    "    if method == 'z-score':\n",
    "        scale_a = preprocessing.scale(a, axis=axis)\n",
    "    elif method== 'minmax':    \n",
    "        scale_a = preprocessing.minmax_scale(a, axis=axis, feature_range=minmax_range) #default feature range 0~1\n",
    "    return scale_a\n",
    "axis =0\n",
    "scale_a1 = normalize(a, axis, method = 'z-score')\n",
    "scale_a2 = normalize(a, axis, method = 'minmax', minmax_range=(0,1))\n",
    "print(scale_a1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 探索性資料分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 散布圖\n",
    "'''\n",
    "color = \"rbg\"\n",
    "color = [color[y[i]] for i in range(len(y))]\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.title('Actual')\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=prediction)\n",
    "plt.title('Prediction')\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 長條圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature correlation\n",
    "\n",
    "colormap = plt.cm.RdBu\n",
    "plt.figure(figsize=(14,12))\n",
    "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
    "sns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \n",
    "            square=True, cmap=colormap, linecolor='white', annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplots\n",
    "\n",
    "g = sns.pairplot(train[[u'Survived', u'Pclass', u'Sex', u'Age', u'Parch', u'Fare', u'Embarked',\n",
    "       u'FamilySize', u'Title']], hue='Survived', palette = 'seismic',size=1.2,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\n",
    "g.set(xticklabels=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 評估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 迭代 train model - 避免imbalance (test data不用拆，直接看AUC&ROC分數即可)\n",
    "\n",
    "'''\n",
    "def train_batch_generator(x, y, bs):\n",
    "    badIndex = y[y == 1].index\n",
    "    goodIndex = y[y == 0].index\n",
    "\n",
    "    while(True):\n",
    "        newBad_ind = shuffle(badIndex)\n",
    "        newgood_ind = shuffle(goodIndex)\n",
    "\n",
    "        newBad_ind = newBad_ind[:int(bs/2)]\n",
    "        newgood_ind = newgood_ind[:int(bs/2)]\n",
    "\n",
    "        batch_x = x.loc[newBad_ind]\n",
    "        batch_y = y.loc[newBad_ind]\n",
    "\n",
    "        batch_x = batch_x.append(x.loc[newgood_ind], ignore_index=True)\n",
    "        batch_y = batch_y.append(y.loc[newgood_ind], ignore_index=True)\n",
    "\n",
    "        yield batch_x, batch_y\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ngen = train_batch_generator(x_train, y_train, 10000)\\nbat_x, bat_y = next(gen)\\nprint(bat_x, bat_y)\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分成 train & test\n",
    "\n",
    "train = pd.merge(train_all, train, on=['FileID'], how='left')\n",
    "train = train.sort_values(['FileID'])\n",
    "\n",
    "x = train.drop(['FileID','label'], axis=1)\n",
    "y = train['label']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state= 68, shuffle=False)\n",
    "\n",
    "'''\n",
    "gen = train_batch_generator(x_train, y_train, 10000)\n",
    "bat_x, bat_y = next(gen)\n",
    "print(bat_x, bat_y)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr = y_train.to_frame()\n",
    "y_te = y_test.to_frame()\n",
    "\n",
    "train_tr = pd.concat([x_train, y_tr], axis=1, join='outer')\n",
    "train_te = pd.concat([x_test,  y_te], axis=1, join='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35181, 84)\n",
      "(35181,)\n",
      "(17329, 84)\n",
      "(17329,)\n",
      "(35181, 85)\n",
      "(17329, 85)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "print(train_tr.shape)\n",
    "print(train_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Regular_x</th>\n",
       "      <th>Regular_y</th>\n",
       "      <th>Delta_sd_x</th>\n",
       "      <th>Delta_sd_y</th>\n",
       "      <th>0374c4</th>\n",
       "      <th>055649</th>\n",
       "      <th>05b409</th>\n",
       "      <th>0cdb7a</th>\n",
       "      <th>20f8a5</th>\n",
       "      <th>...</th>\n",
       "      <th>DayTPrMin</th>\n",
       "      <th>DayTPrMea</th>\n",
       "      <th>DaySPrMax</th>\n",
       "      <th>DaySPrMin</th>\n",
       "      <th>DaySPrMea</th>\n",
       "      <th>TCs</th>\n",
       "      <th>TPr</th>\n",
       "      <th>Day</th>\n",
       "      <th>Fil</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52443</th>\n",
       "      <td>52443</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.170553</td>\n",
       "      <td>0.999552</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>245</td>\n",
       "      <td>1</td>\n",
       "      <td>17.269231</td>\n",
       "      <td>354</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>449</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52444</th>\n",
       "      <td>52444</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.039173</td>\n",
       "      <td>1.436399</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>162</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52445</th>\n",
       "      <td>52445</td>\n",
       "      <td>0.003597</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3.166667</td>\n",
       "      <td>189</td>\n",
       "      <td>1</td>\n",
       "      <td>43.947368</td>\n",
       "      <td>835</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>835</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52446</th>\n",
       "      <td>52446</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.023400</td>\n",
       "      <td>1.023400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52447</th>\n",
       "      <td>52447</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.528375</td>\n",
       "      <td>1.052461</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>6.600000</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  Regular_x  Regular_y  Delta_sd_x  Delta_sd_y  0374c4  \\\n",
       "52443       52443   0.015625        1.0    0.170553    0.999552     0.0   \n",
       "52444       52444   0.130435        1.0    1.039173    1.436399     0.0   \n",
       "52445       52445   0.003597        1.0    0.000000    0.000000     0.0   \n",
       "52446       52446   0.000000        0.0    1.023400    1.023400     0.0   \n",
       "52447       52447   0.406250        1.0    0.528375    1.052461     0.0   \n",
       "\n",
       "       055649  05b409  0cdb7a  20f8a5  ...    DayTPrMin  DayTPrMea  DaySPrMax  \\\n",
       "52443    26.0     0.0     0.0     0.0  ...            2   4.333333        245   \n",
       "52444    25.0     0.0     0.0     0.0  ...            2   3.000000         54   \n",
       "52445     3.0     0.0     0.0     0.0  ...            2   3.166667        189   \n",
       "52446     0.0     0.0     0.0     0.0  ...            1   1.000000         32   \n",
       "52447     0.0     0.0     0.0     0.0  ...            1   2.500000         19   \n",
       "\n",
       "       DaySPrMin  DaySPrMea  TCs  TPr  Day  Fil  label  \n",
       "52443          1  17.269231  354    7    6  449      1  \n",
       "52444          1   9.000000   21    4    6  162      0  \n",
       "52445          1  43.947368  835    4    6  835      0  \n",
       "52446         32  32.000000    1    1    1   32      1  \n",
       "52447          1   6.600000   11    4    2   33      1  \n",
       "\n",
       "[5 rows x 85 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_te.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogisticRegression()     # logistic\n",
    "Ridge()                  # Ridge\n",
    "Lasso()                  # Lasso\n",
    "DecisionTreeClassifier() # Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.82465851  0.77392795  0.80954103  0.81134081  0.76431997]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nmodel = SGDClassifier(loss='log', penalty='l2', alpha=0.000001, l1_ratio=0.15, \\n                      fit_intercept=True, max_iter=None, tol=None, shuffle=True, verbose=0, \\n                      epsilon=0.2,n_jobs=1, random_state=123, learning_rate='optimal', eta0=0, \\n                      power_t=0.2, class_weight='balanced', warm_start=False, average=False, n_iter=2000)\\nmodel.fit(x_train, y_train)\\n\\nprobs = model.predict_proba(x_test)\\nauc   = metrics.roc_auc_score(y_test, [x[1] for x in probs])\\nprint(auc)\\n\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SGD\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model = SGDClassifier(loss='hinge', verbose=False, max_iter=1000, class_weight='balanced')\n",
    "scores = cross_val_score(model, x_train, y_train, cv=5, scoring='roc_auc') # cv: \n",
    "print(scores)\n",
    "\n",
    "'''\n",
    "model = SGDClassifier(loss='log', penalty='l2', alpha=0.000001, l1_ratio=0.15, \n",
    "                      fit_intercept=True, max_iter=None, tol=None, shuffle=True, verbose=0, \n",
    "                      epsilon=0.2,n_jobs=1, random_state=123, learning_rate='optimal', eta0=0, \n",
    "                      power_t=0.2, class_weight='balanced', warm_start=False, average=False, n_iter=2000)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "probs = model.predict_proba(x_test)\n",
    "auc   = metrics.roc_auc_score(y_test, [x[1] for x in probs])\n",
    "print(auc)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.832757592775\n"
     ]
    }
   ],
   "source": [
    "# momentum\n",
    "model = MLPClassifier(hidden_layer_sizes=(100, ), activation='relu', solver='adam', \n",
    "                      alpha=0.0001, batch_size='auto', learning_rate='constant', \n",
    "                      learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, \n",
    "                      random_state=566, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, \n",
    "                      nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, \n",
    "                      beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "probs = model.predict_proba(x_test)\n",
    "auc   = metrics.roc_auc_score(y_test, [x[1] for x in probs])\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "'''\n",
    "model = SVC(kernel='poly',coef0=0, degree=3)\n",
    "model.fit(x_train, y_train)\n",
    "print(model.roc_auc_score(x_test, y_test))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.920483011854 0.913766705764\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(x_train, y_train \n",
    "          #, eval_set=[(x_train, y_train), (x_test, y_test)]\n",
    "          #, eval_metric='auc'\n",
    "          #, verbose=False\n",
    "         )\n",
    "   \n",
    "probs1 = model.predict_proba(x_train)\n",
    "probs2 = model.predict_proba(x_test)\n",
    "\n",
    "auc1   = metrics.roc_auc_score(y_train, [x[1] for x in probs1])\n",
    "auc2   = metrics.roc_auc_score(y_test, [x[1] for x in probs2])\n",
    "    \n",
    "print(auc1, auc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5A 組合模型 (Ensemble): KFold + Stacking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 真正預測時\\ntrain_xy = train\\ntest_xy  = test\\n\\ntrain_x  = x\\ntrain_y  = y\\n\\ntest_x  = test\\n'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型評估時\n",
    "train_xy = train_tr\n",
    "test_xy  = train_te\n",
    "\n",
    "train_x  = x_train\n",
    "train_y  = y_train\n",
    "\n",
    "test_x  = x_test\n",
    "test_y  = y_test\n",
    "\n",
    "'''\n",
    "# 真正預測時\n",
    "train_xy = train\n",
    "test_xy  = test\n",
    "\n",
    "train_x  = x\n",
    "train_y  = y\n",
    "\n",
    "test_x  = test\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ntrain = train_xy.shape[0] \n",
    "ntest  = test_xy.shape[0]  \n",
    "NFOLD = 5\n",
    "SEED = 0\n",
    "\n",
    "kf = KFold(n_splits= NFOLD, random_state=0)   # 參數中有default值的要\n",
    "\n",
    "class SklearnHelper: \n",
    "\n",
    "    def __init__(self, clf, params, seed=SEED):   # 初始化参数  \n",
    "        params['random_state'] = seed  \n",
    "        self.clf = clf(**params)  \n",
    "\n",
    "    def train(self, x, y):  \n",
    "        self.clf.fit(x, y)  \n",
    "  \n",
    "    def predict(self, x):  \n",
    "        return self.clf.predict(x)  \n",
    "      \n",
    "    def fit(self, x, y):  \n",
    "        return self.clf.fit(x, y)  \n",
    "      \n",
    "    def feature_importances(self, x, y):  \n",
    "        print(self.clf.fit(x, y).feature_importances_)  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7037  7038  7039 ..., 35178 35179 35180] [   0    1    2 ..., 7034 7035 7036]\n",
      "[    0     1     2 ..., 35178 35179 35180] [ 7037  7038  7039 ..., 14070 14071 14072]\n",
      "[    0     1     2 ..., 35178 35179 35180] [14073 14074 14075 ..., 21106 21107 21108]\n",
      "[    0     1     2 ..., 35178 35179 35180] [21109 21110 21111 ..., 28142 28143 28144]\n",
      "[    0     1     2 ..., 28142 28143 28144] [28145 28146 28147 ..., 35178 35179 35180]\n"
     ]
    }
   ],
   "source": [
    "kf.split(train_x)\n",
    "for train_index, test_index in kf.split(train_x):\n",
    "    print(train_index, test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oof(clf, kf_x_tr, kf_y_tr, kf_x_te):\n",
    "    \n",
    "    oof_train    = np.zeros((ntrain,))  \n",
    "    oof_test     = np.zeros((ntest,))  \n",
    "    oof_test_skf = np.empty((NFOLDS, ntest))        # 創建一個內容隨機並且依賴與內存狀態的數  \n",
    "  \n",
    "    index=0\n",
    "    for train_index, test_index in kf.split(train_x):  # kfold，i為次數 \n",
    "        x_tr = kf_x_tr[train_index]  # index就是row的編號\n",
    "        y_tr = kf_y_tr[train_index]  \n",
    "        x_te = kf_x_tr[test_index]\n",
    "  \n",
    "        clf.train(x_tr, y_tr) \n",
    "  \n",
    "        oof_train[test_index]  = clf.predict(x_te) \n",
    "        oof_test_skf[index, :] = clf.predict(kf_x_te) \n",
    "        index += 1\n",
    "  \n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)  \n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5B 設定模型參數 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Random Forest  \n",
    "rf_params = {  \n",
    "    'n_jobs': -1,  \n",
    "    'n_estimators': 500,  \n",
    "    'warm_start': True,   \n",
    "     #'max_features': 0.2,  \n",
    "    'max_depth': 6,  \n",
    "    'min_samples_leaf': 2,  \n",
    "    'max_features' : 'sqrt',  \n",
    "    'verbose': 0  \n",
    "}  \n",
    "  \n",
    "# Extra Trees  \n",
    "et_params = {  \n",
    "    'n_jobs': -1,  \n",
    "    'n_estimators':500,  \n",
    "    #'max_features': 0.5,  \n",
    "    'max_depth': 8,  \n",
    "    'min_samples_leaf': 2,  \n",
    "    'verbose': 0  \n",
    "}  \n",
    "  \n",
    "# AdaBoost   \n",
    "ad_params = {  \n",
    "    'n_estimators': 500,  \n",
    "    'learning_rate' : 0.75  \n",
    "}  \n",
    "  \n",
    "# Gradient Boost  \n",
    "gb_params = {  \n",
    "    'n_estimators': 500,  \n",
    "    #'max_features': 0.2,  \n",
    "    'max_depth': 5,  \n",
    "    'min_samples_leaf': 2,  \n",
    "    'verbose': 0  \n",
    "}  \n",
    "  \n",
    "# SVM  \n",
    "sv_params = {  \n",
    "    'kernel':'poly',  \n",
    "    'C' : 0.025  \n",
    "    }  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5C Stacking 第一層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  7.52030156e-04   3.93923353e-02   8.12895534e-04   2.50997783e-02\n",
      "   4.40477753e-03   0.00000000e+00   2.58722242e-03   0.00000000e+00\n",
      "   4.45869130e-05   2.90104585e-03   1.53417954e-04   3.73357846e-06\n",
      "   0.00000000e+00   0.00000000e+00   4.57033378e-02   5.06917291e-03\n",
      "   5.15835538e-03   1.98860858e-02   2.45135395e-04   1.40301477e-02\n",
      "   1.63399709e-04   1.75305032e-04   7.83697699e-03   0.00000000e+00\n",
      "   0.00000000e+00   9.40999918e-04   1.42358252e-02   6.53566031e-03\n",
      "   0.00000000e+00   4.82984526e-03   7.21177861e-04   9.59817587e-03\n",
      "   0.00000000e+00   1.21379092e-02   9.88523105e-03   7.12139497e-03\n",
      "   9.60219624e-03   4.12418117e-03   4.06963045e-03   5.15233077e-03\n",
      "   3.05484322e-03   6.42481215e-03   1.57793623e-02   9.64549649e-03\n",
      "   6.92016540e-03   5.11054754e-03   5.98985039e-03   7.50162459e-03\n",
      "   5.97963748e-03   4.46897937e-03   3.83672089e-03   4.20061273e-03\n",
      "   3.11550542e-03   4.37653859e-03   4.58177241e-03   5.72965797e-03\n",
      "   4.34246980e-03   1.25180704e-02   2.04328370e-02   4.58853069e-03\n",
      "   1.15812687e-02   4.45062038e-03   6.35329708e-03   8.81874804e-03\n",
      "   7.74416782e-03   2.21515426e-02   2.60517516e-02   3.64921307e-02\n",
      "   2.46038731e-02   8.08318916e-03   2.94563334e-02   4.37039303e-02\n",
      "   7.45493202e-03   8.37904977e-02   2.21081352e-02   2.11807345e-02\n",
      "   2.02269024e-02   1.94269539e-02   2.54281509e-02   2.59882032e-02\n",
      "   3.23687813e-02   1.12820374e-02   8.39614165e-02   1.53200688e-02]\n",
      "[  7.51385939e-04   4.97596734e-02   5.44181224e-03   1.19073018e-02\n",
      "   6.84940179e-03   7.83997099e-05   2.45722115e-03   0.00000000e+00\n",
      "   2.01356395e-03   1.16298664e-03   1.98821956e-03   9.44300795e-05\n",
      "   1.16896091e-04   0.00000000e+00   1.11323728e-02   1.28971664e-02\n",
      "   6.64818766e-03   3.02411565e-03   7.59191815e-04   8.50830679e-03\n",
      "   3.48611799e-03   3.48913598e-03   1.04921090e-03   1.09982488e-04\n",
      "   0.00000000e+00   1.15140334e-03   2.35183002e-03   2.56711322e-03\n",
      "   2.24223403e-06   7.16904712e-03   5.39358139e-03   2.13502482e-03\n",
      "   2.52525005e-04   8.64537200e-03   6.91360425e-03   5.05718060e-03\n",
      "   5.51063091e-03   4.28498303e-03   4.45718054e-03   3.13698914e-03\n",
      "   2.76200540e-03   5.23324293e-03   4.24822111e-03   2.93931060e-03\n",
      "   3.03680041e-03   3.29882926e-03   6.71535327e-03   4.25726968e-03\n",
      "   5.09456698e-03   4.19515384e-03   5.00178500e-03   3.17742692e-03\n",
      "   4.96114635e-03   5.33014214e-03   4.80267319e-03   4.47981386e-03\n",
      "   8.25998238e-03   8.54120633e-03   4.50687941e-03   1.99734108e-03\n",
      "   3.19039855e-03   5.28087935e-03   3.38862323e-03   1.90055080e-03\n",
      "   2.72987973e-03   6.53185427e-03   3.14382467e-02   1.30605344e-02\n",
      "   2.11841530e-02   1.60382924e-02   2.21988025e-02   1.49182527e-02\n",
      "   3.71443639e-02   2.65603879e-02   2.36160795e-02   2.34279788e-02\n",
      "   1.69541456e-02   5.68990824e-03   5.16877448e-02   1.74476850e-02\n",
      "   2.62861761e-02   1.83339373e-02   3.17774618e-01   7.62356886e-03]\n",
      "[ 0.01   0.034  0.     0.04   0.024  0.     0.014  0.     0.     0.02   0.\n",
      "  0.002  0.004  0.     0.018  0.008  0.016  0.01   0.     0.02   0.002\n",
      "  0.004  0.004  0.004  0.     0.014  0.022  0.024  0.     0.02   0.002\n",
      "  0.008  0.002  0.012  0.004  0.006  0.008  0.006  0.006  0.012  0.002\n",
      "  0.006  0.006  0.012  0.008  0.006  0.004  0.006  0.006  0.008  0.008\n",
      "  0.004  0.006  0.01   0.002  0.008  0.012  0.004  0.01   0.016  0.006\n",
      "  0.006  0.01   0.016  0.006  0.032  0.014  0.018  0.034  0.012  0.05\n",
      "  0.026  0.006  0.046  0.018  0.002  0.006  0.018  0.01   0.012  0.082\n",
      "  0.012  0.024  0.01 ]\n",
      "[  3.48599630e-02   5.04889002e-02   4.87702177e-04   4.03173839e-02\n",
      "   3.58371027e-02   0.00000000e+00   1.33252720e-02   0.00000000e+00\n",
      "   0.00000000e+00   1.78636333e-02   0.00000000e+00   4.60503350e-04\n",
      "   1.48935047e-04   0.00000000e+00   3.22903698e-02   3.93834672e-03\n",
      "   7.65372256e-03   1.72462721e-02   4.19273292e-05   2.36930331e-02\n",
      "   1.93864548e-04   9.13513332e-04   5.72018464e-03   1.35849286e-04\n",
      "   0.00000000e+00   9.79883020e-03   2.05779960e-02   1.47126701e-02\n",
      "   0.00000000e+00   6.37345519e-03   1.59099469e-03   8.78798088e-03\n",
      "   1.58480078e-03   3.28715990e-02   7.39845770e-03   1.22312359e-02\n",
      "   9.07354339e-03   1.05153860e-02   9.22838680e-03   1.04673014e-02\n",
      "   1.20109869e-02   9.99889623e-03   9.11729739e-03   1.06114666e-02\n",
      "   9.00376281e-03   7.87352902e-03   8.45781129e-03   6.33455114e-03\n",
      "   1.28420236e-02   9.90851824e-03   6.48333093e-03   6.90658275e-03\n",
      "   7.72738623e-03   7.40190218e-03   7.33748012e-03   8.19148315e-03\n",
      "   6.95768380e-03   1.00232489e-02   1.18919718e-02   1.28054046e-02\n",
      "   1.22633487e-02   1.10972239e-02   1.03012739e-02   8.53609212e-03\n",
      "   1.18323911e-02   2.24956180e-02   1.31349202e-02   2.59162512e-02\n",
      "   1.70364770e-02   8.06809107e-03   1.51963420e-02   2.34623271e-02\n",
      "   7.11008964e-03   4.16021221e-02   7.94044314e-03   3.30695164e-03\n",
      "   1.35938272e-02   2.58750643e-02   4.79002827e-03   2.61762162e-02\n",
      "   1.23383365e-02   1.12152971e-02   1.23107796e-02   3.16860531e-02]\n"
     ]
    }
   ],
   "source": [
    "# 每種算法的特徵重要性\n",
    "\n",
    "rf = SklearnHelper(clf=RandomForestClassifier,     seed=SEED, params=rf_params)  \n",
    "et = SklearnHelper(clf=ExtraTreesClassifier,       seed=SEED, params=et_params)  \n",
    "ad = SklearnHelper(clf=AdaBoostClassifier,         seed=SEED, params=ad_params)  \n",
    "gb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)  \n",
    "# sv = SklearnHelper(clf=SVC,                        seed=SEED, params=sv_params) \n",
    "\n",
    "rf_feature  = rf.feature_importances(train_x, train_y)  \n",
    "et_feature  = et.feature_importances(train_x, train_y)  \n",
    "ad_feature  = ad.feature_importances(train_x, train_y)  \n",
    "gb_feature  = gb.feature_importances(train_x, train_y)\n",
    "# sv_feature  = sv.feature_importances(train_x, train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf_x_tr = x_train.values \n",
    "kf_y_tr = y_train.values\n",
    "kf_x_te = x_test.values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:305: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n"
     ]
    }
   ],
   "source": [
    "# 用各模型預測，結果成為下一層的input  \n",
    "\n",
    "et_oof_train, et_oof_test = get_oof(et, kf_x_tr, kf_y_tr, kf_x_te) # Extra Trees  \n",
    "rf_oof_train, rf_oof_test = get_oof(rf, kf_x_tr, kf_y_tr, kf_x_te) # Random Forest  \n",
    "ad_oof_train, ad_oof_test = get_oof(ad, kf_x_tr, kf_y_tr, kf_x_te) # AdaBoost   \n",
    "gb_oof_train, gb_oof_test = get_oof(gb, kf_x_tr, kf_y_tr, kf_x_te) # Gradient Boost  \n",
    "#sv_oof_train, sv_oof_test = get_oof(sv, kf_x_tr, kf_y_tr, kf_x_te) # Support Vector Classifier\n",
    "\n",
    "x_train_new = np.concatenate(( et_oof_train, rf_oof_train, ad_oof_train, gb_oof_train), axis=1)\n",
    "x_test_new  = np.concatenate(( et_oof_test,  rf_oof_test,  ad_oof_test,  gb_oof_test),  axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  1.]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5D Stacking 第二層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.806244239924 0.779379503203\n"
     ]
    }
   ],
   "source": [
    "model = XGBClassifier()\n",
    "model.fit(x_train_new, y_train)\n",
    "predictions = model.predict(x_test_new)\n",
    "\n",
    "probs1 = model.predict_proba(x_train_new)\n",
    "probs2 = model.predict_proba(x_test_new)\n",
    "\n",
    "auc1   = metrics.roc_auc_score(y_train, [i[1] for i in probs1])\n",
    "auc2   = metrics.roc_auc_score(y_test , [i[1] for i in probs2])\n",
    "    \n",
    "print(auc1, auc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#voting model & results\n",
    "\n",
    "#models.append(('logistic', model_logistic))\n",
    "model_cart = DecisionTreeClassifier()\n",
    "models.append(('cart', model_cart))\n",
    "\n",
    "#model_svc = SVC(kernel='poly',coef0=0 ,degree=3) #coef/degree都暫選default\n",
    "\n",
    "#models.append(('svm', model_svc))\n",
    "\n",
    "#model_RF = RandomForestClassifier()\n",
    "\n",
    "#models.append(('RF', model_RF))\n",
    "model_XGB = XGBClassifier( early_stopping_rounds=6, eval_metric=\"auc\",verbose=True)\n",
    "models.append(('XGB', model_XGB))\n",
    "model_MO = MLPClassifier(hidden_layer_sizes=(100, ), activation='relu', solver='adam', \n",
    "                      alpha=0.0001, batch_size='auto', learning_rate='constant', \n",
    "                      learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, \n",
    "                      random_state=566, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, \n",
    "                      nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, \n",
    "                      beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "models.append(('Momentumn', model_MO))\n",
    "\n",
    "ensemble_model = VotingClassifier(estimators=models)\n",
    "result_voting = cross_val_score(ensemble_model, X, Y, cv=kfold)\n",
    "print('voting result:' + str(result_voting.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 美美AUC 圖像\n",
    "\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "svm_clf = SGDClassifier(loss='hinge', verbose=False, max_iter=2000, class_weight='balanced')\n",
    "svm_clf.fit(x_train, y_train)\n",
    "score_roc = svm_clf.decision_function(x_test)\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, score_roc)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figwidth(5)\n",
    "fig.set_figheight(5)\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label='AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 上傳結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileID = test['FileID']\n",
    "predictions = model.predict(test_x)\n",
    "\n",
    "KaggleSubmission = pd.DataFrame({ 'FileID': FileID,\n",
    "                                  'label': predictions })\n",
    "KaggleSubmission.to_csv(\"KaggleSubmission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import time\n",
    "\n",
    "#list1=[]\n",
    "#def func(i):\n",
    " #   time.sleep(np.random.randint(0,5))\n",
    "  #  list1.append(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
